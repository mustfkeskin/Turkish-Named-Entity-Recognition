{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load packages\"\"\"\n",
    "import gc\n",
    "import numpy as np\n",
    "from validation import compute_f1\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, GRU, Bidirectional, MaxPooling1D, \\\n",
    "    Flatten, concatenate\n",
    "from prepro import readfile, createBatches, createMatrices, iterate_minibatches, addCharInformation, padding, getCasing\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD, Nadam, Adam, RMSprop\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BLSTM(object):\n",
    "    \n",
    "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
    "        \n",
    "        self.epochs = EPOCHS\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
    "        self.lstm_state_size = LSTM_STATE_SIZE\n",
    "        self.conv_size = CONV_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.optimizer = OPTIMIZER\n",
    "        \n",
    "    def convertIOB(self, sentence):\n",
    "        sentence_list = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            wordLabel = []\n",
    "            wordLabel.append(word)\n",
    "            wordLabel.append(\"O\\n\")\n",
    "            sentence_list.append(wordLabel)\n",
    "        self.Sentences = [sentence_list]\n",
    "        \n",
    "    def addCharInfo(self):\n",
    "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
    "        self.Sentences = addCharInformation(self.Sentences)\n",
    "        \n",
    "    def createBatches(self):\n",
    "        \"\"\"Create batches\"\"\"\n",
    "        self.batch, self.batch_len = createBatches(self.set)\n",
    "        \n",
    "    def tag_dataset(self, dataset, model):\n",
    "        \"\"\"Tag data with numerical values\"\"\"\n",
    "        correctLabels = []\n",
    "        predLabels = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, casing, char, labels = data\n",
    "            tokens = np.asarray([tokens])\n",
    "            casing = np.asarray([casing])\n",
    "            char = np.asarray([char])\n",
    "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "            correctLabels.append(labels)\n",
    "            predLabels.append(pred)\n",
    "        return predLabels, correctLabels\n",
    "    \n",
    "    def buildModel(self):\n",
    "        \"\"\"Model layers\"\"\"\n",
    "        \n",
    "        char_size = 52\n",
    "        # character input\n",
    "        character_input = Input(shape=(None, char_size,), name=\"Character_input\")\n",
    "        embed_char_out = TimeDistributed(\n",
    "            Embedding(len(self.char2Idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
    "            character_input)\n",
    "\n",
    "        dropout = Dropout(self.dropout)(embed_char_out)\n",
    "\n",
    "        # CNN\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
    "        maxpool_out = TimeDistributed(MaxPooling1D(char_size), name=\"Maxpool\")(conv1d_out)\n",
    "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
    "        char = Dropout(self.dropout)(char)\n",
    "\n",
    "        # word-level input\n",
    "        words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n",
    "                          trainable=False)(words_input)\n",
    "\n",
    "        # case-info input\n",
    "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
    "                           trainable=False)(casing_input)\n",
    "\n",
    "        # concat & BLSTM\n",
    "        output = concatenate([words, char, casing])\n",
    "        output = Bidirectional(GRU(self.lstm_state_size, \n",
    "                                    return_sequences=True, \n",
    "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
    "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
    "                                   ), name=\"BLSTM\")(output)\n",
    "        \n",
    "        output = Bidirectional(GRU(self.lstm_state_size, \n",
    "                                    return_sequences=True, \n",
    "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
    "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
    "                                   ), name=\"BLSTM2\")(output)\n",
    "        \n",
    "\n",
    "        output = TimeDistributed(Dense(50, activation=\"relu\"),name=\"Time_Dense_Layer\")(output)  # a dense layer as suggested by neuralNer\n",
    "\n",
    "        # CRF Layer\n",
    "        crf = CRF(len(self.label2Idx), sparse_target=True)\n",
    "        out = crf(output)\n",
    "        \n",
    "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[out])\n",
    "        self.model.compile(optimizer=self.optimizer, loss=crf_loss, metrics=[crf.accuracy, crf_viterbi_accuracy])        \n",
    "         \n",
    "        # set up model\n",
    "        self.init_weights = self.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mustafa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1               # paper: 80\n",
    "DROPOUT = 0.3             # paper: 0.68\n",
    "DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n",
    "LSTM_STATE_SIZE = 275     # paper: 275\n",
    "CONV_SIZE = 3             # paper: 3\n",
    "LEARNING_RATE = 0.0105    # paper 0.0105\n",
    "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "\n",
    "import pickle\n",
    "with open('cnn_blstm_wordEmbeddings.pickle', 'rb') as f:\n",
    "    cnn_blstm.wordEmbeddings = pickle.load(f)\n",
    "    \n",
    "with open('cnn_blstm_caseEmbeddings.pickle', 'rb') as f:\n",
    "    cnn_blstm.caseEmbeddings = pickle.load(f)\n",
    "\n",
    "with open('cnn_blstm_word2Idx.pickle', 'rb') as f:\n",
    "    cnn_blstm.word2Idx = pickle.load(f)\n",
    "    \n",
    "with open('cnn_blstm_label2Idx.pickle', 'rb') as f:\n",
    "    cnn_blstm.label2Idx = pickle.load(f)\n",
    "    \n",
    "with open('cnn_blstm_case2Idx.pickle', 'rb') as f:\n",
    "    cnn_blstm.case2Idx = pickle.load(f)\n",
    "    \n",
    "with open('cnn_blstm_char2Idx.pickle', 'rb') as f:\n",
    "    cnn_blstm.char2Idx = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ali', ['A', 'l', 'i'], 'O\\n'],\n",
       " ['ata', ['a', 't', 'a'], 'O\\n'],\n",
       " ['bak', ['b', 'a', 'k'], 'O\\n']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"Ali ata bak\"\n",
    "cnn_blstm.convertIOB(example)\n",
    "cnn_blstm.addCharInfo()\n",
    "cnn_blstm.Sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mustafa/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mustafa/anaconda3/lib/python3.6/site-packages/keras_contrib/layers/crf.py:351: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "prediction_set = padding(createMatrices(cnn_blstm.Sentences,\n",
    "                                        cnn_blstm.word2Idx,\n",
    "                                        cnn_blstm.label2Idx,\n",
    "                                        cnn_blstm.case2Idx,\n",
    "                                        cnn_blstm.char2Idx))\n",
    "prediction_batch, prediction_batch_len = createBatches(prediction_set)\n",
    "cnn_blstm.buildModel()\n",
    "cnn_blstm.model.load_weights('1_0.3_0.25_275_3_0.0105_Nadam_0.8086624203821656.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabels, _ = cnn_blstm.tag_dataset(prediction_batch, cnn_blstm.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
